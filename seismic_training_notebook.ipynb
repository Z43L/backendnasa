{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e8c433",
   "metadata": {},
   "source": [
    "# üèîÔ∏è Entrenamiento Interactivo de Modelos S√≠smicos\n",
    "## Sistema de IA para Monitoreo de Deformaci√≥n S√≠smica\n",
    "\n",
    "Este notebook permite entrenar modelos de machine learning para la predicci√≥n de deformaciones s√≠smicas usando datos de InSAR y otras fuentes.\n",
    "\n",
    "### Caracter√≠sticas:\n",
    "- ‚úÖ Entrenamiento interactivo con visualizaci√≥n en tiempo real\n",
    "- ‚úÖ Soporte para clasificaci√≥n y regresi√≥n\n",
    "- ‚úÖ Carga segmentada de datasets grandes\n",
    "- ‚úÖ M√©tricas de evaluaci√≥n completas\n",
    "- ‚úÖ Guardado autom√°tico de modelos entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318343ec",
   "metadata": {},
   "source": [
    "## 1. üìö Importaci√≥n de Librer√≠as y Configuraci√≥n\n",
    "\n",
    "Importamos todas las librer√≠as necesarias para el entrenamiento de modelos s√≠smicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c34dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n de librer√≠as necesarias\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuraci√≥n del entorno\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# A√±adir el directorio backend al path\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "backend_dir = os.path.join(notebook_dir, 'backend')\n",
    "if backend_dir not in sys.path:\n",
    "    sys.path.insert(0, backend_dir)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(f\"üìÅ Directorio backend: {backend_dir}\")\n",
    "print(f\"üñ•Ô∏è  PyTorch versi√≥n: {torch.__version__}\")\n",
    "print(f\"üéØ CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11c439",
   "metadata": {},
   "source": [
    "## 2. üìä Carga y Exploraci√≥n de Datos S√≠smicos\n",
    "\n",
    "Exploramos los datasets disponibles y configuramos la carga de datos con chunking para manejar datasets grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c50894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploraci√≥n de datasets disponibles\n",
    "def explore_datasets():\n",
    "    \"\"\"Explora los datasets disponibles en el directorio backend/datasets\"\"\"\n",
    "    datasets_dir = os.path.join(backend_dir, 'datasets')\n",
    "\n",
    "    if not os.path.exists(datasets_dir):\n",
    "        print(f\"‚ùå Directorio de datasets no encontrado: {datasets_dir}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"üìÇ Explorando datasets en: {datasets_dir}\")\n",
    "    datasets = []\n",
    "\n",
    "    for file in os.listdir(datasets_dir):\n",
    "        if file.endswith('.h5'):\n",
    "            filepath = os.path.join(datasets_dir, file)\n",
    "            try:\n",
    "                with h5py.File(filepath, 'r') as f:\n",
    "                    info = {\n",
    "                        'filename': file,\n",
    "                        'filepath': filepath,\n",
    "                        'size_mb': os.path.getsize(filepath) / (1024 * 1024),\n",
    "                        'groups': list(f.keys())\n",
    "                    }\n",
    "\n",
    "                    # Obtener informaci√≥n detallada del dataset\n",
    "                    if len(f.keys()) > 0:\n",
    "                        main_group = list(f.keys())[0]\n",
    "                        grupo = f[main_group]\n",
    "                        if 'secuencias' in grupo:\n",
    "                            shape = grupo['secuencias'].shape\n",
    "                            info['shape'] = shape\n",
    "                            info['num_samples'] = shape[0]\n",
    "                            info['seq_length'] = shape[1]\n",
    "                            info['grid_height'] = shape[2]\n",
    "                            info['grid_width'] = shape[3]\n",
    "\n",
    "                        if 'etiquetas' in grupo:\n",
    "                            info['has_labels'] = True\n",
    "                            info['task_type'] = 'classification'\n",
    "                        elif 'regresion' in f:\n",
    "                            info['has_labels'] = True\n",
    "                            info['task_type'] = 'regression'\n",
    "                        else:\n",
    "                            info['has_labels'] = False\n",
    "\n",
    "                    datasets.append(info)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error al leer {file}: {e}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# Ejecutar exploraci√≥n\n",
    "datasets_info = explore_datasets()\n",
    "\n",
    "# Mostrar informaci√≥n de datasets\n",
    "if datasets_info:\n",
    "    print(f\"\\nüìä Encontrados {len(datasets_info)} datasets:\")\n",
    "    for i, ds in enumerate(datasets_info, 1):\n",
    "        print(f\"\\n{i}. {ds['filename']}\")\n",
    "        print(f\"   üìè Tama√±o: {ds['size_mb']:.1f} MB\")\n",
    "        if 'shape' in ds:\n",
    "            print(f\"   üìê Forma: {ds['shape']} (muestras √ó tiempo √ó altura √ó ancho)\")\n",
    "        if 'task_type' in ds:\n",
    "            print(f\"   üéØ Tipo: {ds['task_type']}\")\n",
    "        print(f\"   üìÇ Grupos: {ds['groups']}\")\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron datasets HDF5\")\n",
    "    print(\"üí° Ejecuta 'python main.py generate' en el directorio backend para crear datasets sint√©ticos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para generar datasets autom√°ticamente si no existen\n",
    "def generate_datasets_if_needed():\n",
    "    \"\"\"Genera datasets sint√©ticos autom√°ticamente si no existen\"\"\"\n",
    "    datasets_dir = os.path.join(backend_dir, 'datasets')\n",
    "\n",
    "    # Verificar si ya existen datasets\n",
    "    if os.path.exists(datasets_dir):\n",
    "        existing_files = [f for f in os.listdir(datasets_dir) if f.endswith('.h5')]\n",
    "        if existing_files:\n",
    "            print(f\"‚úÖ Ya existen {len(existing_files)} datasets: {existing_files}\")\n",
    "            return True\n",
    "\n",
    "    print(\"üìä No se encontraron datasets. Generando datos sint√©ticos autom√°ticamente...\")\n",
    "    print(\"üîß Esto puede tomar varios minutos...\")\n",
    "\n",
    "    try:\n",
    "        # Importar y ejecutar la generaci√≥n de datasets\n",
    "        import subprocess\n",
    "        import sys\n",
    "\n",
    "        # Ejecutar el comando de generaci√≥n desde el directorio backend\n",
    "        result = subprocess.run([\n",
    "            sys.executable, 'main.py', 'generate'\n",
    "        ], cwd=backend_dir, capture_output=True, text=True)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Datasets generados exitosamente!\")\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Error al generar datasets:\")\n",
    "            print(result.stderr)\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error ejecutando generaci√≥n de datasets: {e}\")\n",
    "        return False\n",
    "\n",
    "# Generar datasets autom√°ticamente si es necesario\n",
    "datasets_available = generate_datasets_if_needed()\n",
    "\n",
    "if datasets_available:\n",
    "    # Re-explorar datasets despu√©s de generarlos\n",
    "    datasets_info = explore_datasets()\n",
    "else:\n",
    "    print(\"‚ùå No se pudieron generar los datasets. Verifica la configuraci√≥n del backend.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c09c049",
   "metadata": {},
   "source": [
    "## 3. ‚öôÔ∏è Configuraci√≥n del Entrenamiento\n",
    "\n",
    "Configura los par√°metros del entrenamiento usando controles interactivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6124f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widgets interactivos para configuraci√≥n del entrenamiento\n",
    "if datasets_info:\n",
    "    # Selector de dataset\n",
    "    dataset_selector = widgets.Dropdown(\n",
    "        options=[(f\"{ds['filename']} ({ds['task_type']})\", ds['filepath']) for ds in datasets_info],\n",
    "        description='Dataset:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Selector de tarea (si no se puede inferir del dataset)\n",
    "    task_selector = widgets.Dropdown(\n",
    "        options=['classification', 'regression'],\n",
    "        value='classification',\n",
    "        description='Tarea:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Par√°metros de entrenamiento\n",
    "    epochs_slider = widgets.IntSlider(\n",
    "        value=30, min=5, max=200, step=5,\n",
    "        description='√âpocas:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    batch_size_slider = widgets.IntSlider(\n",
    "        value=4, min=1, max=32, step=1,\n",
    "        description='Batch Size:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    learning_rate_slider = widgets.FloatLogSlider(\n",
    "        value=1e-4, base=10, min=-6, max=-2, step=0.5,\n",
    "        description='Learning Rate:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    chunk_size_slider = widgets.IntSlider(\n",
    "        value=1000, min=100, max=5000, step=100,\n",
    "        description='Chunk Size:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Bot√≥n de entrenamiento\n",
    "    train_button = widgets.Button(\n",
    "        description='üöÄ Iniciar Entrenamiento',\n",
    "        button_style='success',\n",
    "        tooltip='Comenzar el entrenamiento del modelo'\n",
    "    )\n",
    "\n",
    "    # √Årea de salida\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Mostrar widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h4>üìä Configuraci√≥n del Dataset</h4>\"),\n",
    "        dataset_selector,\n",
    "        task_selector,\n",
    "        widgets.HTML(\"<h4>üîß Par√°metros de Entrenamiento</h4>\"),\n",
    "        epochs_slider,\n",
    "        batch_size_slider,\n",
    "        learning_rate_slider,\n",
    "        chunk_size_slider,\n",
    "        widgets.HTML(\"<h4>üéØ Control del Entrenamiento</h4>\"),\n",
    "        train_button,\n",
    "        output_area\n",
    "    ]))\n",
    "\n",
    "    # Funci√≥n para determinar la tarea autom√°ticamente\n",
    "    def update_task_selector(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            selected_path = change['new']\n",
    "            for ds in datasets_info:\n",
    "                if ds['filepath'] == selected_path:\n",
    "                    if 'task_type' in ds:\n",
    "                        task_selector.value = ds['task_type']\n",
    "                    break\n",
    "\n",
    "    dataset_selector.observe(update_task_selector)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No hay datasets disponibles. Genera datasets primero ejecutando:\")\n",
    "    print(\"   cd backend && python main.py generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ecaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de entrenamiento autom√°tico con par√°metros por defecto\n",
    "def train_model_auto():\n",
    "    \"\"\"Entrenamiento autom√°tico con par√°metros optimizados\"\"\"\n",
    "    if not datasets_info:\n",
    "        print(\"‚ùå No hay datasets disponibles para entrenar\")\n",
    "        return\n",
    "\n",
    "    # Usar el primer dataset disponible (clasificaci√≥n por defecto)\n",
    "    selected_dataset = None\n",
    "    for ds in datasets_info:\n",
    "        if 'classification' in ds.get('task_type', ''):\n",
    "            selected_dataset = ds\n",
    "            break\n",
    "\n",
    "    if not selected_dataset:\n",
    "        selected_dataset = datasets_info[0]  # Usar cualquier dataset disponible\n",
    "\n",
    "    print(\"üöÄ Iniciando entrenamiento autom√°tico...\"    print(f\"üìä Dataset seleccionado: {selected_dataset['filename']}\")\n",
    "    print(f\"üéØ Tarea: {selected_dataset.get('task_type', 'classification')}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Par√°metros optimizados por defecto\n",
    "    config = {\n",
    "        'dataset_path': selected_dataset['filepath'],\n",
    "        'task_type': selected_dataset.get('task_type', 'classification'),\n",
    "        'epochs': 50,\n",
    "        'batch_size': 8,\n",
    "        'learning_rate': 1e-4,\n",
    "        'chunk_size': 1000\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Crear dataset\n",
    "        dataset = DeformationDataset(\n",
    "            config['dataset_path'],\n",
    "            task_type=config['task_type'],\n",
    "            chunk_size=config['chunk_size']\n",
    "        )\n",
    "\n",
    "        # Dividir en train/val\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        # Crear dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "        # Crear modelo\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = create_model(\n",
    "            d_model=256,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=6,\n",
    "            seq_length=dataset.seq_length,\n",
    "            grid_size=(dataset.grid_size[0], dataset.grid_size[1]),\n",
    "            num_classes=3 if config['task_type'] == 'classification' else 1,\n",
    "            task_type=config['task_type']\n",
    "        ).to(device)\n",
    "\n",
    "        print(f\"üìä Modelo creado: {sum(p.numel() for p in model.parameters())} par√°metros\")\n",
    "        print(f\"üñ•Ô∏è  Dispositivo: {device}\")\n",
    "\n",
    "        # Configurar optimizador y loss\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss() if config['task_type'] == 'classification' else nn.MSELoss()\n",
    "\n",
    "        # Entrenamiento\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = [] if config['task_type'] == 'classification' else []\n",
    "        val_accuracies = [] if config['task_type'] == 'classification' else []\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "\n",
    "        print(\"üèÉ Iniciando entrenamiento autom√°tico...\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for epoch in range(config['epochs']):\n",
    "            # Entrenamiento\n",
    "            model.train()\n",
    "            epoch_train_loss = 0\n",
    "            epoch_train_correct = 0\n",
    "            epoch_train_total = 0\n",
    "\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "\n",
    "                if config['task_type'] == 'classification':\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    epoch_train_correct += (predicted == batch_y).sum().item()\n",
    "                    epoch_train_total += batch_y.size(0)\n",
    "                else:\n",
    "                    loss = criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "            train_accuracy = epoch_train_correct / epoch_train_total if epoch_train_total > 0 else 0\n",
    "\n",
    "            # Validaci√≥n\n",
    "            model.eval()\n",
    "            epoch_val_loss = 0\n",
    "            epoch_val_correct = 0\n",
    "            epoch_val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_x)\n",
    "\n",
    "                    if config['task_type'] == 'classification':\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        epoch_val_correct += (predicted == batch_y).sum().item()\n",
    "                        epoch_val_total += batch_y.size(0)\n",
    "                    else:\n",
    "                        loss = criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "                    epoch_val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "            val_accuracy = epoch_val_correct / epoch_val_total if epoch_val_total > 0 else 0\n",
    "\n",
    "            # Guardar m√©tricas\n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            if config['task_type'] == 'classification':\n",
    "                train_accuracies.append(train_accuracy)\n",
    "                val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Mostrar progreso cada 5 √©pocas\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"√âpoca {epoch+1:3d}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "                if config['task_type'] == 'classification':\n",
    "                    print(f\"            Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Guardar mejor modelo\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                checkpoint_path = f\"checkpoints_{config['task_type']}_auto_{timestamp}_best.pth\"\n",
    "                save_model_checkpoint(model, optimizer, epoch, best_val_loss, checkpoint_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"‚èπÔ∏è  Early stopping en √©poca {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        print(\"‚úÖ Entrenamiento autom√°tico completado!\")\n",
    "\n",
    "        # Visualizar resultados\n",
    "        plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies, config['task_type'])\n",
    "\n",
    "        # Mostrar resumen final\n",
    "        print(\"\n",
    "üìä Resumen del Entrenamiento Autom√°tico:\"        print(f\"   Modelo guardado: {checkpoint_path}\")\n",
    "        print(f\"   √âpocas completadas: {len(train_losses)}\")\n",
    "        print(f\"   Mejor p√©rdida de validaci√≥n: {best_val_loss:.4f}\")\n",
    "        if config['task_type'] == 'classification' and val_accuracies:\n",
    "            print(f\"   Mejor precisi√≥n de validaci√≥n: {max(val_accuracies):.4f}\")\n",
    "\n",
    "        return checkpoint_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en entrenamiento autom√°tico: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Bot√≥n de entrenamiento autom√°tico\n",
    "auto_train_button = widgets.Button(\n",
    "    description='ü§ñ Entrenamiento Autom√°tico',\n",
    "    button_style='success',\n",
    "    tooltip='Entrenar autom√°ticamente con par√°metros optimizados'\n",
    ")\n",
    "\n",
    "auto_output_area = widgets.Output()\n",
    "\n",
    "def auto_train_click(b):\n",
    "    with auto_output_area:\n",
    "        clear_output(wait=True)\n",
    "        train_model_auto()\n",
    "\n",
    "auto_train_button.on_click(auto_train_click)\n",
    "\n",
    "# Mostrar bot√≥n de entrenamiento autom√°tico\n",
    "if datasets_info:\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h4>üöÄ Entrenamiento Autom√°tico</h4>\"),\n",
    "        widgets.HTML(\"Entrenamiento completo con par√°metros optimizados por defecto\"),\n",
    "        auto_train_button,\n",
    "        auto_output_area\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c141ce9",
   "metadata": {},
   "source": [
    "## 4. üöÄ Entrenamiento del Modelo\n",
    "\n",
    "Ejecuta el entrenamiento con visualizaci√≥n en tiempo real del progreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar funciones del backend\n",
    "try:\n",
    "    from model_architecture import create_model, save_model_checkpoint, load_model_checkpoint\n",
    "    from model_training.train_model import DeformationDataset\n",
    "    print(\"‚úÖ Funciones del backend importadas correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error al importar funciones del backend: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el directorio backend est√© en el path\")\n",
    "\n",
    "# Funci√≥n de entrenamiento interactivo\n",
    "def train_model_interactive(b):\n",
    "    \"\"\"Funci√≥n que maneja el entrenamiento cuando se presiona el bot√≥n\"\"\"\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        try:\n",
    "            # Obtener configuraci√≥n\n",
    "            dataset_path = dataset_selector.value\n",
    "            task_type = task_selector.value\n",
    "            epochs = epochs_slider.value\n",
    "            batch_size = batch_size_slider.value\n",
    "            learning_rate = learning_rate_slider.value\n",
    "            chunk_size = chunk_size_slider.value\n",
    "\n",
    "            print(\"üöÄ Iniciando entrenamiento interactivo...\"            print(f\"üìä Dataset: {os.path.basename(dataset_path)}\")\n",
    "            print(f\"üéØ Tarea: {task_type}\")\n",
    "            print(f\"üìà √âpocas: {epochs}\")\n",
    "            print(f\"üì¶ Batch size: {batch_size}\")\n",
    "            print(f\"üéì Learning rate: {learning_rate}\")\n",
    "            print(f\"üß© Chunk size: {chunk_size}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Crear dataset\n",
    "            print(\"üìö Creando dataset...\")\n",
    "            dataset = DeformationDataset(dataset_path, task_type=task_type, chunk_size=chunk_size)\n",
    "\n",
    "            # Dividir en train/val\n",
    "            train_size = int(0.8 * len(dataset))\n",
    "            val_size = len(dataset) - train_size\n",
    "            train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "            # Crear dataloaders\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            print(f\"‚úÖ Dataset creado: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
    "\n",
    "            # Crear modelo\n",
    "            print(\"üèóÔ∏è  Creando modelo...\")\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            model = create_model(\n",
    "                d_model=256,\n",
    "                nhead=8,\n",
    "                num_encoder_layers=6,\n",
    "                seq_length=dataset.seq_length,\n",
    "                grid_size=(dataset.grid_size[0], dataset.grid_size[1]),\n",
    "                num_classes=3 if task_type == 'classification' else 1,\n",
    "                task_type=task_type\n",
    "            ).to(device)\n",
    "\n",
    "            print(f\"üìä Modelo creado con {sum(p.numel() for p in model.parameters())} par√°metros\")\n",
    "            print(f\"üñ•Ô∏è  Dispositivo: {device}\")\n",
    "\n",
    "            # Configurar optimizador y loss\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            if task_type == 'classification':\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "            else:\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "            # Entrenamiento\n",
    "            print(\"üèÉ Iniciando entrenamiento...\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Listas para m√©tricas\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            train_accuracies = [] if task_type == 'classification' else []\n",
    "            val_accuracies = [] if task_type == 'classification' else []\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            patience = 10\n",
    "            patience_counter = 0\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                # Entrenamiento\n",
    "                model.train()\n",
    "                epoch_train_loss = 0\n",
    "                epoch_train_correct = 0\n",
    "                epoch_train_total = 0\n",
    "\n",
    "                train_pbar = tqdm(train_loader, desc=f'√âpoca {epoch+1}/{epochs} [Train]')\n",
    "                for batch_x, batch_y in train_pbar:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_x)\n",
    "\n",
    "                    if task_type == 'classification':\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        epoch_train_correct += (predicted == batch_y).sum().item()\n",
    "                        epoch_train_total += batch_y.size(0)\n",
    "                    else:\n",
    "                        loss = criterion(outputs.squeeze(), batch_y)\n",
    "                        # Para regresi√≥n, calculamos una \"precisi√≥n\" basada en error relativo\n",
    "                        pred_flat = outputs.squeeze().view(-1)\n",
    "                        true_flat = batch_y.view(-1)\n",
    "                        accuracy = (torch.abs(pred_flat - true_flat) / (torch.abs(true_flat) + 1e-8) < 0.1).float().mean().item()\n",
    "                        epoch_train_correct += accuracy * batch_y.numel()\n",
    "                        epoch_train_total += batch_y.numel()\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_train_loss += loss.item()\n",
    "                    train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "                avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "                train_accuracy = epoch_train_correct / epoch_train_total if epoch_train_total > 0 else 0\n",
    "\n",
    "                # Validaci√≥n\n",
    "                model.eval()\n",
    "                epoch_val_loss = 0\n",
    "                epoch_val_correct = 0\n",
    "                epoch_val_total = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_pbar = tqdm(val_loader, desc=f'√âpoca {epoch+1}/{epochs} [Val]')\n",
    "                    for batch_x, batch_y in val_pbar:\n",
    "                        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                        outputs = model(batch_x)\n",
    "\n",
    "                        if task_type == 'classification':\n",
    "                            loss = criterion(outputs, batch_y)\n",
    "                            _, predicted = torch.max(outputs.data, 1)\n",
    "                            epoch_val_correct += (predicted == batch_y).sum().item()\n",
    "                            epoch_val_total += batch_y.size(0)\n",
    "                        else:\n",
    "                            loss = criterion(outputs.squeeze(), batch_y)\n",
    "                            pred_flat = outputs.squeeze().view(-1)\n",
    "                            true_flat = batch_y.view(-1)\n",
    "                            accuracy = (torch.abs(pred_flat - true_flat) / (torch.abs(true_flat) + 1e-8) < 0.1).float().mean().item()\n",
    "                            epoch_val_correct += accuracy * batch_y.numel()\n",
    "                            epoch_val_total += batch_y.numel()\n",
    "\n",
    "                        epoch_val_loss += loss.item()\n",
    "                        val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "                avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "                val_accuracy = epoch_val_correct / epoch_val_total if epoch_val_total > 0 else 0\n",
    "\n",
    "                # Guardar m√©tricas\n",
    "                train_losses.append(avg_train_loss)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                if task_type == 'classification':\n",
    "                    train_accuracies.append(train_accuracy)\n",
    "                    val_accuracies.append(val_accuracy)\n",
    "\n",
    "                # Mostrar progreso\n",
    "                print(f\"√âpoca {epoch+1:3d}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "                if task_type == 'classification':\n",
    "                    print(f\"            Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "                else:\n",
    "                    print(f\"            Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "                # Early stopping\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    patience_counter = 0\n",
    "                    # Guardar mejor modelo\n",
    "                    checkpoint_path = f\"checkpoints_{task_type}_{os.path.basename(dataset_path).split('.')[0]}_best.pth\"\n",
    "                    save_model_checkpoint(model, optimizer, epoch, best_val_loss, checkpoint_path)\n",
    "                    print(f\"üíæ Mejor modelo guardado: {checkpoint_path}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"‚èπÔ∏è  Early stopping en √©poca {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            print(\"‚úÖ Entrenamiento completado!\")\n",
    "\n",
    "            # Visualizar resultados\n",
    "            plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies, task_type)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error durante el entrenamiento: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Conectar el bot√≥n a la funci√≥n\n",
    "if 'train_button' in locals():\n",
    "    train_button.on_click(train_model_interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeacb8e",
   "metadata": {},
   "source": [
    "## 5. üìà Visualizaci√≥n de Resultados\n",
    "\n",
    "Visualiza las m√©tricas de entrenamiento y eval√∫a el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff75aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para visualizar resultados del entrenamiento\n",
    "def plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies, task_type):\n",
    "    \"\"\"Visualiza las curvas de entrenamiento\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2 if task_type == 'classification' else 1,\n",
    "        subplot_titles=['P√©rdida (Loss)', 'Precisi√≥n (Accuracy)' if task_type == 'classification' else None],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}] if task_type == 'classification' else [{\"secondary_y\": False}]]\n",
    "    )\n",
    "\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "    # Gr√°fico de p√©rdida\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=train_losses, mode='lines+markers', name='Train Loss',\n",
    "                  line=dict(color='blue', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=val_losses, mode='lines+markers', name='Val Loss',\n",
    "                  line=dict(color='red', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Gr√°fico de precisi√≥n (solo para clasificaci√≥n)\n",
    "    if task_type == 'classification' and train_accuracies and val_accuracies:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=train_accuracies, mode='lines+markers', name='Train Accuracy',\n",
    "                      line=dict(color='green', width=2)),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=val_accuracies, mode='lines+markers', name='Val Accuracy',\n",
    "                      line=dict(color='orange', width=2)),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "    # Configurar layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"üìä Resultados del Entrenamiento\",\n",
    "        title_x=0.5,\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"√âpoca\")\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "    if task_type == 'classification':\n",
    "        fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Mostrar m√©tricas finales\n",
    "    print(\"\n",
    "üìä M√©tricas Finales:\"    print(f\"   P√©rdida de entrenamiento final: {train_losses[-1]:.4f}\")\n",
    "    print(f\"   P√©rdida de validaci√≥n final: {val_losses[-1]:.4f}\")\n",
    "    if task_type == 'classification' and train_accuracies and val_accuracies:\n",
    "        print(f\"   Precisi√≥n de entrenamiento final: {train_accuracies[-1]:.4f}\")\n",
    "        print(f\"   Precisi√≥n de validaci√≥n final: {val_accuracies[-1]:.4f}\")\n",
    "\n",
    "    # Encontrar mejores m√©tricas\n",
    "    best_epoch = val_losses.index(min(val_losses)) + 1\n",
    "    print(f\"   Mejor √©poca: {best_epoch}\")\n",
    "    print(f\"   Mejor p√©rdida de validaci√≥n: {min(val_losses):.4f}\")\n",
    "\n",
    "# Funci√≥n para evaluar el modelo en detalle\n",
    "def evaluate_model_detailed(model_path, dataset_path, task_type):\n",
    "    \"\"\"Evaluaci√≥n detallada del modelo\"\"\"\n",
    "    try:\n",
    "        print(f\"üîç Evaluando modelo: {model_path}\")\n",
    "\n",
    "        # Cargar modelo\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        checkpoint = load_model_checkpoint(model_path, device)\n",
    "\n",
    "        # Crear dataset de prueba\n",
    "        dataset = DeformationDataset(dataset_path, task_type=task_type, chunk_size=1000)\n",
    "\n",
    "        # Usar el 20% final para prueba\n",
    "        test_size = int(0.2 * len(dataset))\n",
    "        train_val_size = len(dataset) - test_size\n",
    "        _, test_dataset = random_split(dataset, [train_val_size, test_size])\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "        # Evaluar\n",
    "        model = checkpoint['model']\n",
    "        model.eval()\n",
    "\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in tqdm(test_loader, desc=\"Evaluando\"):\n",
    "                batch_x = batch_x.to(device)\n",
    "                outputs = model(batch_x)\n",
    "\n",
    "                if task_type == 'classification':\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    all_predictions.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(batch_y.numpy())\n",
    "                else:\n",
    "                    all_predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "                    all_labels.extend(batch_y.numpy())\n",
    "\n",
    "        # Calcular m√©tricas\n",
    "        if task_type == 'classification':\n",
    "            from sklearn.metrics import classification_report, confusion_matrix\n",
    "            print(\"\\nüìä Reporte de Clasificaci√≥n:\")\n",
    "            print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "            # Matriz de confusi√≥n\n",
    "            cm = confusion_matrix(all_labels, all_predictions)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=['Precursor', 'Normal', 'Post-terremoto'],\n",
    "                       yticklabels=['Precursor', 'Normal', 'Post-terremoto'])\n",
    "            plt.title('Matriz de Confusi√≥n')\n",
    "            plt.ylabel('Verdadero')\n",
    "            plt.xlabel('Predicho')\n",
    "            plt.show()\n",
    "        else:\n",
    "            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "            mse = mean_squared_error(all_labels, all_predictions)\n",
    "            mae = mean_absolute_error(all_labels, all_predictions)\n",
    "            r2 = r2_score(all_labels, all_predictions)\n",
    "\n",
    "            print(\"\n",
    "üìä M√©tricas de Regresi√≥n:\"            print(f\"   MSE: {mse:.4f}\")\n",
    "            print(f\"   MAE: {mae:.4f}\")\n",
    "            print(f\"   R¬≤: {r2:.4f}\")\n",
    "\n",
    "            # Gr√°fico de predicci√≥n vs real\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(all_labels, all_predictions, alpha=0.5)\n",
    "            plt.plot([min(all_labels), max(all_labels)], [min(all_labels), max(all_labels)], 'r--')\n",
    "            plt.xlabel('Valores Reales')\n",
    "            plt.ylabel('Predicciones')\n",
    "            plt.title('Predicci√≥n vs Real')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en evaluaci√≥n detallada: {e}\")\n",
    "\n",
    "print(\"‚úÖ Funciones de visualizaci√≥n y evaluaci√≥n cargadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb4998",
   "metadata": {},
   "source": [
    "## 6. üìù Instrucciones de Uso\n",
    "\n",
    "### üöÄ Inicio R√°pido:\n",
    "1. **Ejecuta todas las celdas** en orden (Cell ‚Üí Run All)\n",
    "2. **Configura los par√°metros** usando los controles interactivos\n",
    "3. **Presiona \"üöÄ Iniciar Entrenamiento\"** para comenzar\n",
    "4. **Monitorea el progreso** en tiempo real\n",
    "5. **Revisa los resultados** en las visualizaciones\n",
    "\n",
    "### üìä Datasets Disponibles:\n",
    "- **falla_anatolia**: Datos de la falla de Anatolia (Turqu√≠a)\n",
    "- **cinturon_fuego_pacifico**: Datos del cintur√≥n de fuego del Pac√≠fico\n",
    "\n",
    "### üéØ Tipos de Tarea:\n",
    "- **classification**: Predice si es precursor, normal, o post-terremoto\n",
    "- **regression**: Predice valores continuos de deformaci√≥n\n",
    "\n",
    "### ‚öôÔ∏è Par√°metros Recomendados:\n",
    "- **√âpocas**: 30-100 (dependiendo del dataset)\n",
    "- **Batch Size**: 4-16 (ajusta seg√∫n memoria disponible)\n",
    "- **Learning Rate**: 1e-4 a 1e-3\n",
    "- **Chunk Size**: 1000-2000 (para carga eficiente)\n",
    "\n",
    "### üíæ Modelos Guardados:\n",
    "Los modelos se guardan autom√°ticamente en el directorio `backend/checkpoints_*` con el mejor rendimiento en validaci√≥n.\n",
    "\n",
    "### üîß Soluci√≥n de Problemas:\n",
    "- Si no hay datasets: ejecuta `cd backend && python main.py generate`\n",
    "- Si hay errores de memoria: reduce batch_size o chunk_size\n",
    "- Si el entrenamiento es lento: verifica que CUDA est√© disponible\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ ¬°Listo para Entrenar!\n",
    "\n",
    "Este notebook proporciona una interfaz completa y interactiva para entrenar modelos de predicci√≥n s√≠smica. ¬°Experimenta con diferentes configuraciones y datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43ae6d",
   "metadata": {},
   "source": [
    "## 7. ‚ö° Modo Autom√°tico Completo\n",
    "\n",
    "Ejecuta todo el proceso autom√°ticamente: generaci√≥n de datos + entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0276e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para ejecutar todo el proceso autom√°ticamente\n",
    "def run_complete_auto_pipeline():\n",
    "    \"\"\"Ejecuta el pipeline completo: generaci√≥n de datos + entrenamiento autom√°tico\"\"\"\n",
    "    print(\"üöÄ Iniciando Pipeline Autom√°tico Completo\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Paso 1: Generar datasets si no existen\n",
    "    print(\"üìä PASO 1: Generaci√≥n de Datasets\")\n",
    "    datasets_ok = generate_datasets_if_needed()\n",
    "\n",
    "    if not datasets_ok:\n",
    "        print(\"‚ùå Error en generaci√≥n de datasets. Abortando pipeline.\")\n",
    "        return False\n",
    "\n",
    "    # Re-explorar datasets\n",
    "    global datasets_info\n",
    "    datasets_info = explore_datasets()\n",
    "\n",
    "    if not datasets_info:\n",
    "        print(\"‚ùå No se encontraron datasets despu√©s de la generaci√≥n. Abortando pipeline.\")\n",
    "        return False\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ü§ñ PASO 2: Entrenamiento Autom√°tico\")\n",
    "    # Paso 2: Entrenamiento autom√°tico\n",
    "    model_path = train_model_auto()\n",
    "\n",
    "    if model_path:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ PIPELINE COMPLETADO EXITOSAMENTE!\")\n",
    "        print(f\"üìÅ Modelo guardado en: {model_path}\")\n",
    "        print(\"\\nüéØ El modelo est√° listo para usar en inferencia!\")\n",
    "        print(\"üí° Puedes usar el modelo con: python main.py seismic\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Error en el entrenamiento. Pipeline incompleto.\")\n",
    "        return False\n",
    "\n",
    "# Bot√≥n para pipeline completo autom√°tico\n",
    "complete_auto_button = widgets.Button(\n",
    "    description='üöÄ Pipeline Completo Autom√°tico',\n",
    "    button_style='primary',\n",
    "    tooltip='Generar datos + entrenar autom√°ticamente (todo en uno)',\n",
    "    layout=widgets.Layout(width='300px', height='50px')\n",
    ")\n",
    "\n",
    "complete_output_area = widgets.Output()\n",
    "\n",
    "def complete_auto_click(b):\n",
    "    with complete_output_area:\n",
    "        clear_output(wait=True)\n",
    "        run_complete_auto_pipeline()\n",
    "\n",
    "complete_auto_button.on_click(complete_auto_click)\n",
    "\n",
    "# Mostrar bot√≥n de pipeline completo\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>‚ö° Pipeline Autom√°tico Completo</h3>\"),\n",
    "    widgets.HTML(\"üîß Genera datasets sint√©ticos autom√°ticamente<br>ü§ñ Entrena modelo con par√°metros optimizados<br>üìä Visualiza resultados completos\"),\n",
    "    widgets.HTML(\"<br><strong>¬°Solo presiona el bot√≥n y espera!</strong>\"),\n",
    "    complete_auto_button,\n",
    "    complete_output_area\n",
    "]))\n",
    "\n",
    "print(\"üéØ El notebook est√° listo para usar en modo autom√°tico o interactivo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95a2ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Estado del Sistema\n",
    "\n",
    "Ejecuta esta celda para verificar el estado actual del sistema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99749d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificaci√≥n del estado del sistema\n",
    "def check_system_status():\n",
    "    \"\"\"Verifica el estado completo del sistema de entrenamiento s√≠smico\"\"\"\n",
    "    print(\"üîç Verificando estado del sistema...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Verificar backend\n",
    "    backend_ok = os.path.exists(backend_dir)\n",
    "    print(f\"üìÅ Backend: {'‚úÖ' if backend_ok else '‚ùå'} ({backend_dir})\")\n",
    "\n",
    "    # Verificar datasets\n",
    "    datasets_dir = os.path.join(backend_dir, 'datasets')\n",
    "    datasets_ok = os.path.exists(datasets_dir)\n",
    "    dataset_files = []\n",
    "    if datasets_ok:\n",
    "        dataset_files = [f for f in os.listdir(datasets_dir) if f.endswith('.h5')]\n",
    "    print(f\"üìä Datasets: {'‚úÖ' if datasets_ok and dataset_files else '‚ùå'} ({len(dataset_files)} archivos)\")\n",
    "\n",
    "    # Verificar modelos\n",
    "    models_dir = os.path.join(backend_dir, 'models')\n",
    "    models_ok = os.path.exists(models_dir)\n",
    "    print(f\"ü§ñ Modelos: {'‚úÖ' if models_ok else '‚ùå'} ({models_dir})\")\n",
    "\n",
    "    # Verificar checkpoints\n",
    "    checkpoints_dir = os.path.join(backend_dir, 'checkpoints')\n",
    "    checkpoints_ok = os.path.exists(checkpoints_dir)\n",
    "    checkpoint_files = []\n",
    "    if checkpoints_ok:\n",
    "        checkpoint_files = [f for f in os.listdir(checkpoints_dir) if f.endswith('.pth')]\n",
    "    print(f\"üíæ Checkpoints: {'‚úÖ' if checkpoints_ok else '‚ùå'} ({len(checkpoint_files)} modelos)\")\n",
    "\n",
    "    # Verificar PyTorch y CUDA\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    device_count = torch.cuda.device_count() if cuda_available else 0\n",
    "    print(f\"üñ•Ô∏è  PyTorch: ‚úÖ (CUDA: {'‚úÖ' if cuda_available else '‚ùå'}, GPUs: {device_count})\")\n",
    "\n",
    "    # Verificar imports cr√≠ticos\n",
    "    imports_ok = True\n",
    "    try:\n",
    "        from model_architecture import create_model\n",
    "        from model_training.train_model import DeformationDataset\n",
    "        print(\"üìö Imports: ‚úÖ (model_architecture, train_model)\")\n",
    "    except ImportError as e:\n",
    "        print(f\"üìö Imports: ‚ùå ({e})\")\n",
    "        imports_ok = False\n",
    "\n",
    "    # Resumen\n",
    "    all_ok = backend_ok and imports_ok\n",
    "    datasets_ready = datasets_ok and dataset_files\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìã RESUMEN DEL SISTEMA:\")\n",
    "    print(f\"   Backend configurado: {'‚úÖ' if all_ok else '‚ùå'}\")\n",
    "    print(f\"   Datasets listos: {'‚úÖ' if datasets_ready else '‚ùå'}\")\n",
    "    print(f\"   Entrenamiento posible: {'‚úÖ' if all_ok and (datasets_ready or True) else '‚ùå'}\")\n",
    "\n",
    "    if all_ok and not datasets_ready:\n",
    "        print(\"   üí° Los datasets se generar√°n autom√°ticamente en modo autom√°tico\")\n",
    "    elif all_ok and datasets_ready:\n",
    "        print(\"   üéØ Sistema completamente listo para entrenamiento!\")\n",
    "\n",
    "    return all_ok\n",
    "\n",
    "# Ejecutar verificaci√≥n\n",
    "system_ready = check_system_status()\n",
    "\n",
    "if system_ready:\n",
    "    print(\"\\nüéâ ¬°Sistema listo! Elige tu modo de entrenamiento:\")\n",
    "    print(\"   üöÄ Pipeline Completo Autom√°tico (recomendado)\")\n",
    "    print(\"   üéõÔ∏è  Modo Interactivo (control total)\")\n",
    "    print(\"   ü§ñ Entrenamiento Autom√°tico (par√°metros optimizados)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Sistema necesita configuraci√≥n. Verifica los errores arriba.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
