{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e8c433",
   "metadata": {},
   "source": [
    "# 🏔️ Entrenamiento Interactivo de Modelos Sísmicos\n",
    "## Sistema de IA para Monitoreo de Deformación Sísmica\n",
    "\n",
    "Este notebook permite entrenar modelos de machine learning para la predicción de deformaciones sísmicas usando datos de InSAR y otras fuentes.\n",
    "\n",
    "### Características:\n",
    "- ✅ Entrenamiento interactivo con visualización en tiempo real\n",
    "- ✅ Soporte para clasificación y regresión\n",
    "- ✅ Carga segmentada de datasets grandes\n",
    "- ✅ Métricas de evaluación completas\n",
    "- ✅ Guardado automático de modelos entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318343ec",
   "metadata": {},
   "source": [
    "## 1. 📚 Importación de Librerías y Configuración\n",
    "\n",
    "Importamos todas las librerías necesarias para el entrenamiento de modelos sísmicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c34dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuración del entorno\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Añadir el directorio backend al path\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "backend_dir = os.path.join(notebook_dir, 'backend')\n",
    "if backend_dir not in sys.path:\n",
    "    sys.path.insert(0, backend_dir)\n",
    "\n",
    "print(\"✅ Librerías importadas correctamente\")\n",
    "print(f\"📁 Directorio backend: {backend_dir}\")\n",
    "print(f\"🖥️  PyTorch versión: {torch.__version__}\")\n",
    "print(f\"🎯 CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11c439",
   "metadata": {},
   "source": [
    "## 2. 📊 Carga y Exploración de Datos Sísmicos\n",
    "\n",
    "Exploramos los datasets disponibles y configuramos la carga de datos con chunking para manejar datasets grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c50894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploración de datasets disponibles\n",
    "def explore_datasets():\n",
    "    \"\"\"Explora los datasets disponibles en el directorio backend/datasets\"\"\"\n",
    "    datasets_dir = os.path.join(backend_dir, 'datasets')\n",
    "\n",
    "    if not os.path.exists(datasets_dir):\n",
    "        print(f\"❌ Directorio de datasets no encontrado: {datasets_dir}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"📂 Explorando datasets en: {datasets_dir}\")\n",
    "    datasets = []\n",
    "\n",
    "    for file in os.listdir(datasets_dir):\n",
    "        if file.endswith('.h5'):\n",
    "            filepath = os.path.join(datasets_dir, file)\n",
    "            try:\n",
    "                with h5py.File(filepath, 'r') as f:\n",
    "                    info = {\n",
    "                        'filename': file,\n",
    "                        'filepath': filepath,\n",
    "                        'size_mb': os.path.getsize(filepath) / (1024 * 1024),\n",
    "                        'groups': list(f.keys())\n",
    "                    }\n",
    "\n",
    "                    # Obtener información detallada del dataset\n",
    "                    if len(f.keys()) > 0:\n",
    "                        main_group = list(f.keys())[0]\n",
    "                        grupo = f[main_group]\n",
    "                        if 'secuencias' in grupo:\n",
    "                            shape = grupo['secuencias'].shape\n",
    "                            info['shape'] = shape\n",
    "                            info['num_samples'] = shape[0]\n",
    "                            info['seq_length'] = shape[1]\n",
    "                            info['grid_height'] = shape[2]\n",
    "                            info['grid_width'] = shape[3]\n",
    "\n",
    "                        if 'etiquetas' in grupo:\n",
    "                            info['has_labels'] = True\n",
    "                            info['task_type'] = 'classification'\n",
    "                        elif 'regresion' in f:\n",
    "                            info['has_labels'] = True\n",
    "                            info['task_type'] = 'regression'\n",
    "                        else:\n",
    "                            info['has_labels'] = False\n",
    "\n",
    "                    datasets.append(info)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error al leer {file}: {e}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# Ejecutar exploración\n",
    "datasets_info = explore_datasets()\n",
    "\n",
    "# Mostrar información de datasets\n",
    "if datasets_info:\n",
    "    print(f\"\\n📊 Encontrados {len(datasets_info)} datasets:\")\n",
    "    for i, ds in enumerate(datasets_info, 1):\n",
    "        print(f\"\\n{i}. {ds['filename']}\")\n",
    "        print(f\"   📏 Tamaño: {ds['size_mb']:.1f} MB\")\n",
    "        if 'shape' in ds:\n",
    "            print(f\"   📐 Forma: {ds['shape']} (muestras × tiempo × altura × ancho)\")\n",
    "        if 'task_type' in ds:\n",
    "            print(f\"   🎯 Tipo: {ds['task_type']}\")\n",
    "        print(f\"   📂 Grupos: {ds['groups']}\")\n",
    "else:\n",
    "    print(\"❌ No se encontraron datasets HDF5\")\n",
    "    print(\"💡 Ejecuta 'python main.py generate' en el directorio backend para crear datasets sintéticos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar datasets automáticamente si no existen\n",
    "def generate_datasets_if_needed():\n",
    "    \"\"\"Genera datasets sintéticos automáticamente si no existen\"\"\"\n",
    "    datasets_dir = os.path.join(backend_dir, 'datasets')\n",
    "\n",
    "    # Verificar si ya existen datasets\n",
    "    if os.path.exists(datasets_dir):\n",
    "        existing_files = [f for f in os.listdir(datasets_dir) if f.endswith('.h5')]\n",
    "        if existing_files:\n",
    "            print(f\"✅ Ya existen {len(existing_files)} datasets: {existing_files}\")\n",
    "            return True\n",
    "\n",
    "    print(\"📊 No se encontraron datasets. Generando datos sintéticos automáticamente...\")\n",
    "    print(\"🔧 Esto puede tomar varios minutos...\")\n",
    "\n",
    "    try:\n",
    "        # Importar y ejecutar la generación de datasets\n",
    "        import subprocess\n",
    "        import sys\n",
    "\n",
    "        # Ejecutar el comando de generación desde el directorio backend\n",
    "        result = subprocess.run([\n",
    "            sys.executable, 'main.py', 'generate'\n",
    "        ], cwd=backend_dir, capture_output=True, text=True)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Datasets generados exitosamente!\")\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Error al generar datasets:\")\n",
    "            print(result.stderr)\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error ejecutando generación de datasets: {e}\")\n",
    "        return False\n",
    "\n",
    "# Generar datasets automáticamente si es necesario\n",
    "datasets_available = generate_datasets_if_needed()\n",
    "\n",
    "if datasets_available:\n",
    "    # Re-explorar datasets después de generarlos\n",
    "    datasets_info = explore_datasets()\n",
    "else:\n",
    "    print(\"❌ No se pudieron generar los datasets. Verifica la configuración del backend.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c09c049",
   "metadata": {},
   "source": [
    "## 3. ⚙️ Configuración del Entrenamiento\n",
    "\n",
    "Configura los parámetros del entrenamiento usando controles interactivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6124f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widgets interactivos para configuración del entrenamiento\n",
    "if datasets_info:\n",
    "    # Selector de dataset\n",
    "    dataset_selector = widgets.Dropdown(\n",
    "        options=[(f\"{ds['filename']} ({ds['task_type']})\", ds['filepath']) for ds in datasets_info],\n",
    "        description='Dataset:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Selector de tarea (si no se puede inferir del dataset)\n",
    "    task_selector = widgets.Dropdown(\n",
    "        options=['classification', 'regression'],\n",
    "        value='classification',\n",
    "        description='Tarea:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Parámetros de entrenamiento\n",
    "    epochs_slider = widgets.IntSlider(\n",
    "        value=30, min=5, max=200, step=5,\n",
    "        description='Épocas:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    batch_size_slider = widgets.IntSlider(\n",
    "        value=4, min=1, max=32, step=1,\n",
    "        description='Batch Size:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    learning_rate_slider = widgets.FloatLogSlider(\n",
    "        value=1e-4, base=10, min=-6, max=-2, step=0.5,\n",
    "        description='Learning Rate:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    chunk_size_slider = widgets.IntSlider(\n",
    "        value=1000, min=100, max=5000, step=100,\n",
    "        description='Chunk Size:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Botón de entrenamiento\n",
    "    train_button = widgets.Button(\n",
    "        description='🚀 Iniciar Entrenamiento',\n",
    "        button_style='success',\n",
    "        tooltip='Comenzar el entrenamiento del modelo'\n",
    "    )\n",
    "\n",
    "    # Área de salida\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Mostrar widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h4>📊 Configuración del Dataset</h4>\"),\n",
    "        dataset_selector,\n",
    "        task_selector,\n",
    "        widgets.HTML(\"<h4>🔧 Parámetros de Entrenamiento</h4>\"),\n",
    "        epochs_slider,\n",
    "        batch_size_slider,\n",
    "        learning_rate_slider,\n",
    "        chunk_size_slider,\n",
    "        widgets.HTML(\"<h4>🎯 Control del Entrenamiento</h4>\"),\n",
    "        train_button,\n",
    "        output_area\n",
    "    ]))\n",
    "\n",
    "    # Función para determinar la tarea automáticamente\n",
    "    def update_task_selector(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            selected_path = change['new']\n",
    "            for ds in datasets_info:\n",
    "                if ds['filepath'] == selected_path:\n",
    "                    if 'task_type' in ds:\n",
    "                        task_selector.value = ds['task_type']\n",
    "                    break\n",
    "\n",
    "    dataset_selector.observe(update_task_selector)\n",
    "\n",
    "else:\n",
    "    print(\"❌ No hay datasets disponibles. Genera datasets primero ejecutando:\")\n",
    "    print(\"   cd backend && python main.py generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ecaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento automático con parámetros por defecto\n",
    "def train_model_auto():\n",
    "    \"\"\"Entrenamiento automático con parámetros optimizados\"\"\"\n",
    "    if not datasets_info:\n",
    "        print(\"❌ No hay datasets disponibles para entrenar\")\n",
    "        return\n",
    "\n",
    "    # Usar el primer dataset disponible (clasificación por defecto)\n",
    "    selected_dataset = None\n",
    "    for ds in datasets_info:\n",
    "        if 'classification' in ds.get('task_type', ''):\n",
    "            selected_dataset = ds\n",
    "            break\n",
    "\n",
    "    if not selected_dataset:\n",
    "        selected_dataset = datasets_info[0]  # Usar cualquier dataset disponible\n",
    "\n",
    "    print(\"🚀 Iniciando entrenamiento automático...\"    print(f\"📊 Dataset seleccionado: {selected_dataset['filename']}\")\n",
    "    print(f\"🎯 Tarea: {selected_dataset.get('task_type', 'classification')}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Parámetros optimizados por defecto\n",
    "    config = {\n",
    "        'dataset_path': selected_dataset['filepath'],\n",
    "        'task_type': selected_dataset.get('task_type', 'classification'),\n",
    "        'epochs': 50,\n",
    "        'batch_size': 8,\n",
    "        'learning_rate': 1e-4,\n",
    "        'chunk_size': 1000\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Crear dataset\n",
    "        dataset = DeformationDataset(\n",
    "            config['dataset_path'],\n",
    "            task_type=config['task_type'],\n",
    "            chunk_size=config['chunk_size']\n",
    "        )\n",
    "\n",
    "        # Dividir en train/val\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        # Crear dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "        # Crear modelo\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = create_model(\n",
    "            d_model=256,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=6,\n",
    "            seq_length=dataset.seq_length,\n",
    "            grid_size=(dataset.grid_size[0], dataset.grid_size[1]),\n",
    "            num_classes=3 if config['task_type'] == 'classification' else 1,\n",
    "            task_type=config['task_type']\n",
    "        ).to(device)\n",
    "\n",
    "        print(f\"📊 Modelo creado: {sum(p.numel() for p in model.parameters())} parámetros\")\n",
    "        print(f\"🖥️  Dispositivo: {device}\")\n",
    "\n",
    "        # Configurar optimizador y loss\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss() if config['task_type'] == 'classification' else nn.MSELoss()\n",
    "\n",
    "        # Entrenamiento\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = [] if config['task_type'] == 'classification' else []\n",
    "        val_accuracies = [] if config['task_type'] == 'classification' else []\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "\n",
    "        print(\"🏃 Iniciando entrenamiento automático...\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for epoch in range(config['epochs']):\n",
    "            # Entrenamiento\n",
    "            model.train()\n",
    "            epoch_train_loss = 0\n",
    "            epoch_train_correct = 0\n",
    "            epoch_train_total = 0\n",
    "\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "\n",
    "                if config['task_type'] == 'classification':\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    epoch_train_correct += (predicted == batch_y).sum().item()\n",
    "                    epoch_train_total += batch_y.size(0)\n",
    "                else:\n",
    "                    loss = criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "            train_accuracy = epoch_train_correct / epoch_train_total if epoch_train_total > 0 else 0\n",
    "\n",
    "            # Validación\n",
    "            model.eval()\n",
    "            epoch_val_loss = 0\n",
    "            epoch_val_correct = 0\n",
    "            epoch_val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                    outputs = model(batch_x)\n",
    "\n",
    "                    if config['task_type'] == 'classification':\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        epoch_val_correct += (predicted == batch_y).sum().item()\n",
    "                        epoch_val_total += batch_y.size(0)\n",
    "                    else:\n",
    "                        loss = criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "                    epoch_val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "            val_accuracy = epoch_val_correct / epoch_val_total if epoch_val_total > 0 else 0\n",
    "\n",
    "            # Guardar métricas\n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            if config['task_type'] == 'classification':\n",
    "                train_accuracies.append(train_accuracy)\n",
    "                val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Mostrar progreso cada 5 épocas\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"Época {epoch+1:3d}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "                if config['task_type'] == 'classification':\n",
    "                    print(f\"            Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Guardar mejor modelo\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                checkpoint_path = f\"checkpoints_{config['task_type']}_auto_{timestamp}_best.pth\"\n",
    "                save_model_checkpoint(model, optimizer, epoch, best_val_loss, checkpoint_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"⏹️  Early stopping en época {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        print(\"✅ Entrenamiento automático completado!\")\n",
    "\n",
    "        # Visualizar resultados\n",
    "        plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies, config['task_type'])\n",
    "\n",
    "        # Mostrar resumen final\n",
    "        print(\"\n",
    "📊 Resumen del Entrenamiento Automático:\"        print(f\"   Modelo guardado: {checkpoint_path}\")\n",
    "        print(f\"   Épocas completadas: {len(train_losses)}\")\n",
    "        print(f\"   Mejor pérdida de validación: {best_val_loss:.4f}\")\n",
    "        if config['task_type'] == 'classification' and val_accuracies:\n",
    "            print(f\"   Mejor precisión de validación: {max(val_accuracies):.4f}\")\n",
    "\n",
    "        return checkpoint_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en entrenamiento automático: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Botón de entrenamiento automático\n",
    "auto_train_button = widgets.Button(\n",
    "    description='🤖 Entrenamiento Automático',\n",
    "    button_style='success',\n",
    "    tooltip='Entrenar automáticamente con parámetros optimizados'\n",
    ")\n",
    "\n",
    "auto_output_area = widgets.Output()\n",
    "\n",
    "def auto_train_click(b):\n",
    "    with auto_output_area:\n",
    "        clear_output(wait=True)\n",
    "        train_model_auto()\n",
    "\n",
    "auto_train_button.on_click(auto_train_click)\n",
    "\n",
    "# Mostrar botón de entrenamiento automático\n",
    "if datasets_info:\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h4>🚀 Entrenamiento Automático</h4>\"),\n",
    "        widgets.HTML(\"Entrenamiento completo con parámetros optimizados por defecto\"),\n",
    "        auto_train_button,\n",
    "        auto_output_area\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c141ce9",
   "metadata": {},
   "source": [
    "## 4. 🚀 Entrenamiento del Modelo\n",
    "\n",
    "Ejecuta el entrenamiento con visualización en tiempo real del progreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar funciones del backend\n",
    "try:\n",
    "    from model_architecture import create_model, save_model_checkpoint, load_model_checkpoint\n",
    "    from model_training.train_model import DeformationDataset\n",
    "    print(\"✅ Funciones del backend importadas correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error al importar funciones del backend: {e}\")\n",
    "    print(\"💡 Asegúrate de que el directorio backend esté en el path\")\n",
    "\n",
    "# Función de entrenamiento interactivo\n",
    "def train_model_interactive(b):\n",
    "    \"\"\"Función que maneja el entrenamiento cuando se presiona el botón\"\"\"\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        try:\n",
    "            # Obtener configuración\n",
    "            dataset_path = dataset_selector.value\n",
    "            task_type = task_selector.value\n",
    "            epochs = epochs_slider.value\n",
    "            batch_size = batch_size_slider.value\n",
    "            learning_rate = learning_rate_slider.value\n",
    "            chunk_size = chunk_size_slider.value\n",
    "\n",
    "            print(\"🚀 Iniciando entrenamiento interactivo...\"            print(f\"📊 Dataset: {os.path.basename(dataset_path)}\")\n",
    "            print(f\"🎯 Tarea: {task_type}\")\n",
    "            print(f\"📈 Épocas: {epochs}\")\n",
    "            print(f\"📦 Batch size: {batch_size}\")\n",
    "            print(f\"🎓 Learning rate: {learning_rate}\")\n",
    "            print(f\"🧩 Chunk size: {chunk_size}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Crear dataset\n",
    "            print(\"📚 Creando dataset...\")\n",
    "            dataset = DeformationDataset(dataset_path, task_type=task_type, chunk_size=chunk_size)\n",
    "\n",
    "            # Dividir en train/val\n",
    "            train_size = int(0.8 * len(dataset))\n",
    "            val_size = len(dataset) - train_size\n",
    "            train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "            # Crear dataloaders\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            print(f\"✅ Dataset creado: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
    "\n",
    "            # Crear modelo\n",
    "            print(\"🏗️  Creando modelo...\")\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            model = create_model(\n",
    "                d_model=256,\n",
    "                nhead=8,\n",
    "                num_encoder_layers=6,\n",
    "                seq_length=dataset.seq_length,\n",
    "                grid_size=(dataset.grid_size[0], dataset.grid_size[1]),\n",
    "                num_classes=3 if task_type == 'classification' else 1,\n",
    "                task_type=task_type\n",
    "            ).to(device)\n",
    "\n",
    "            print(f\"📊 Modelo creado con {sum(p.numel() for p in model.parameters())} parámetros\")\n",
    "            print(f\"🖥️  Dispositivo: {device}\")\n",
    "\n",
    "            # Configurar optimizador y loss\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            if task_type == 'classification':\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "            else:\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "            # Entrenamiento\n",
    "            print(\"🏃 Iniciando entrenamiento...\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Listas para métricas\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            train_accuracies = [] if task_type == 'classification' else []\n",
    "            val_accuracies = [] if task_type == 'classification' else []\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            patience = 10\n",
    "            patience_counter = 0\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                # Entrenamiento\n",
    "                model.train()\n",
    "                epoch_train_loss = 0\n",
    "                epoch_train_correct = 0\n",
    "                epoch_train_total = 0\n",
    "\n",
    "                train_pbar = tqdm(train_loader, desc=f'Época {epoch+1}/{epochs} [Train]')\n",
    "                for batch_x, batch_y in train_pbar:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_x)\n",
    "\n",
    "                    if task_type == 'classification':\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        epoch_train_correct += (predicted == batch_y).sum().item()\n",
    "                        epoch_train_total += batch_y.size(0)\n",
    "                    else:\n",
    "                        loss = criterion(outputs.squeeze(), batch_y)\n",
    "                        # Para regresión, calculamos una \"precisión\" basada en error relativo\n",
    "                        pred_flat = outputs.squeeze().view(-1)\n",
    "                        true_flat = batch_y.view(-1)\n",
    "                        accuracy = (torch.abs(pred_flat - true_flat) / (torch.abs(true_flat) + 1e-8) < 0.1).float().mean().item()\n",
    "                        epoch_train_correct += accuracy * batch_y.numel()\n",
    "                        epoch_train_total += batch_y.numel()\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_train_loss += loss.item()\n",
    "                    train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "                avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "                train_accuracy = epoch_train_correct / epoch_train_total if epoch_train_total > 0 else 0\n",
    "\n",
    "                # Validación\n",
    "                model.eval()\n",
    "                epoch_val_loss = 0\n",
    "                epoch_val_correct = 0\n",
    "                epoch_val_total = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_pbar = tqdm(val_loader, desc=f'Época {epoch+1}/{epochs} [Val]')\n",
    "                    for batch_x, batch_y in val_pbar:\n",
    "                        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                        outputs = model(batch_x)\n",
    "\n",
    "                        if task_type == 'classification':\n",
    "                            loss = criterion(outputs, batch_y)\n",
    "                            _, predicted = torch.max(outputs.data, 1)\n",
    "                            epoch_val_correct += (predicted == batch_y).sum().item()\n",
    "                            epoch_val_total += batch_y.size(0)\n",
    "                        else:\n",
    "                            loss = criterion(outputs.squeeze(), batch_y)\n",
    "                            pred_flat = outputs.squeeze().view(-1)\n",
    "                            true_flat = batch_y.view(-1)\n",
    "                            accuracy = (torch.abs(pred_flat - true_flat) / (torch.abs(true_flat) + 1e-8) < 0.1).float().mean().item()\n",
    "                            epoch_val_correct += accuracy * batch_y.numel()\n",
    "                            epoch_val_total += batch_y.numel()\n",
    "\n",
    "                        epoch_val_loss += loss.item()\n",
    "                        val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "                avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "                val_accuracy = epoch_val_correct / epoch_val_total if epoch_val_total > 0 else 0\n",
    "\n",
    "                # Guardar métricas\n",
    "                train_losses.append(avg_train_loss)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                if task_type == 'classification':\n",
    "                    train_accuracies.append(train_accuracy)\n",
    "                    val_accuracies.append(val_accuracy)\n",
    "\n",
    "                # Mostrar progreso\n",
    "                print(f\"Época {epoch+1:3d}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "                if task_type == 'classification':\n",
    "                    print(f\"            Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "                else:\n",
    "                    print(f\"            Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "                # Early stopping\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    patience_counter = 0\n",
    "                    # Guardar mejor modelo\n",
    "                    checkpoint_path = f\"checkpoints_{task_type}_{os.path.basename(dataset_path).split('.')[0]}_best.pth\"\n",
    "                    save_model_checkpoint(model, optimizer, epoch, best_val_loss, checkpoint_path)\n",
    "                    print(f\"💾 Mejor modelo guardado: {checkpoint_path}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"⏹️  Early stopping en época {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            print(\"✅ Entrenamiento completado!\")\n",
    "\n",
    "            # Visualizar resultados\n",
    "            plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies, task_type)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error durante el entrenamiento: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Conectar el botón a la función\n",
    "if 'train_button' in locals():\n",
    "    train_button.on_click(train_model_interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeacb8e",
   "metadata": {},
   "source": [
    "## 5. 📈 Visualización de Resultados\n",
    "\n",
    "Visualiza las métricas de entrenamiento y evalúa el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff75aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar resultados del entrenamiento\n",
    "def plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies, task_type):\n",
    "    \"\"\"Visualiza las curvas de entrenamiento\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2 if task_type == 'classification' else 1,\n",
    "        subplot_titles=['Pérdida (Loss)', 'Precisión (Accuracy)' if task_type == 'classification' else None],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}] if task_type == 'classification' else [{\"secondary_y\": False}]]\n",
    "    )\n",
    "\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "    # Gráfico de pérdida\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=train_losses, mode='lines+markers', name='Train Loss',\n",
    "                  line=dict(color='blue', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=val_losses, mode='lines+markers', name='Val Loss',\n",
    "                  line=dict(color='red', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Gráfico de precisión (solo para clasificación)\n",
    "    if task_type == 'classification' and train_accuracies and val_accuracies:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=train_accuracies, mode='lines+markers', name='Train Accuracy',\n",
    "                      line=dict(color='green', width=2)),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=val_accuracies, mode='lines+markers', name='Val Accuracy',\n",
    "                      line=dict(color='orange', width=2)),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "    # Configurar layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"📊 Resultados del Entrenamiento\",\n",
    "        title_x=0.5,\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Época\")\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "    if task_type == 'classification':\n",
    "        fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Mostrar métricas finales\n",
    "    print(\"\n",
    "📊 Métricas Finales:\"    print(f\"   Pérdida de entrenamiento final: {train_losses[-1]:.4f}\")\n",
    "    print(f\"   Pérdida de validación final: {val_losses[-1]:.4f}\")\n",
    "    if task_type == 'classification' and train_accuracies and val_accuracies:\n",
    "        print(f\"   Precisión de entrenamiento final: {train_accuracies[-1]:.4f}\")\n",
    "        print(f\"   Precisión de validación final: {val_accuracies[-1]:.4f}\")\n",
    "\n",
    "    # Encontrar mejores métricas\n",
    "    best_epoch = val_losses.index(min(val_losses)) + 1\n",
    "    print(f\"   Mejor época: {best_epoch}\")\n",
    "    print(f\"   Mejor pérdida de validación: {min(val_losses):.4f}\")\n",
    "\n",
    "# Función para evaluar el modelo en detalle\n",
    "def evaluate_model_detailed(model_path, dataset_path, task_type):\n",
    "    \"\"\"Evaluación detallada del modelo\"\"\"\n",
    "    try:\n",
    "        print(f\"🔍 Evaluando modelo: {model_path}\")\n",
    "\n",
    "        # Cargar modelo\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        checkpoint = load_model_checkpoint(model_path, device)\n",
    "\n",
    "        # Crear dataset de prueba\n",
    "        dataset = DeformationDataset(dataset_path, task_type=task_type, chunk_size=1000)\n",
    "\n",
    "        # Usar el 20% final para prueba\n",
    "        test_size = int(0.2 * len(dataset))\n",
    "        train_val_size = len(dataset) - test_size\n",
    "        _, test_dataset = random_split(dataset, [train_val_size, test_size])\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "        # Evaluar\n",
    "        model = checkpoint['model']\n",
    "        model.eval()\n",
    "\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in tqdm(test_loader, desc=\"Evaluando\"):\n",
    "                batch_x = batch_x.to(device)\n",
    "                outputs = model(batch_x)\n",
    "\n",
    "                if task_type == 'classification':\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    all_predictions.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(batch_y.numpy())\n",
    "                else:\n",
    "                    all_predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "                    all_labels.extend(batch_y.numpy())\n",
    "\n",
    "        # Calcular métricas\n",
    "        if task_type == 'classification':\n",
    "            from sklearn.metrics import classification_report, confusion_matrix\n",
    "            print(\"\\n📊 Reporte de Clasificación:\")\n",
    "            print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "            # Matriz de confusión\n",
    "            cm = confusion_matrix(all_labels, all_predictions)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=['Precursor', 'Normal', 'Post-terremoto'],\n",
    "                       yticklabels=['Precursor', 'Normal', 'Post-terremoto'])\n",
    "            plt.title('Matriz de Confusión')\n",
    "            plt.ylabel('Verdadero')\n",
    "            plt.xlabel('Predicho')\n",
    "            plt.show()\n",
    "        else:\n",
    "            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "            mse = mean_squared_error(all_labels, all_predictions)\n",
    "            mae = mean_absolute_error(all_labels, all_predictions)\n",
    "            r2 = r2_score(all_labels, all_predictions)\n",
    "\n",
    "            print(\"\n",
    "📊 Métricas de Regresión:\"            print(f\"   MSE: {mse:.4f}\")\n",
    "            print(f\"   MAE: {mae:.4f}\")\n",
    "            print(f\"   R²: {r2:.4f}\")\n",
    "\n",
    "            # Gráfico de predicción vs real\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(all_labels, all_predictions, alpha=0.5)\n",
    "            plt.plot([min(all_labels), max(all_labels)], [min(all_labels), max(all_labels)], 'r--')\n",
    "            plt.xlabel('Valores Reales')\n",
    "            plt.ylabel('Predicciones')\n",
    "            plt.title('Predicción vs Real')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en evaluación detallada: {e}\")\n",
    "\n",
    "print(\"✅ Funciones de visualización y evaluación cargadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb4998",
   "metadata": {},
   "source": [
    "## 6. 📝 Instrucciones de Uso\n",
    "\n",
    "### 🚀 Inicio Rápido:\n",
    "1. **Ejecuta todas las celdas** en orden (Cell → Run All)\n",
    "2. **Configura los parámetros** usando los controles interactivos\n",
    "3. **Presiona \"🚀 Iniciar Entrenamiento\"** para comenzar\n",
    "4. **Monitorea el progreso** en tiempo real\n",
    "5. **Revisa los resultados** en las visualizaciones\n",
    "\n",
    "### 📊 Datasets Disponibles:\n",
    "- **falla_anatolia**: Datos de la falla de Anatolia (Turquía)\n",
    "- **cinturon_fuego_pacifico**: Datos del cinturón de fuego del Pacífico\n",
    "\n",
    "### 🎯 Tipos de Tarea:\n",
    "- **classification**: Predice si es precursor, normal, o post-terremoto\n",
    "- **regression**: Predice valores continuos de deformación\n",
    "\n",
    "### ⚙️ Parámetros Recomendados:\n",
    "- **Épocas**: 30-100 (dependiendo del dataset)\n",
    "- **Batch Size**: 4-16 (ajusta según memoria disponible)\n",
    "- **Learning Rate**: 1e-4 a 1e-3\n",
    "- **Chunk Size**: 1000-2000 (para carga eficiente)\n",
    "\n",
    "### 💾 Modelos Guardados:\n",
    "Los modelos se guardan automáticamente en el directorio `backend/checkpoints_*` con el mejor rendimiento en validación.\n",
    "\n",
    "### 🔧 Solución de Problemas:\n",
    "- Si no hay datasets: ejecuta `cd backend && python main.py generate`\n",
    "- Si hay errores de memoria: reduce batch_size o chunk_size\n",
    "- Si el entrenamiento es lento: verifica que CUDA esté disponible\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 ¡Listo para Entrenar!\n",
    "\n",
    "Este notebook proporciona una interfaz completa y interactiva para entrenar modelos de predicción sísmica. ¡Experimenta con diferentes configuraciones y datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43ae6d",
   "metadata": {},
   "source": [
    "## 7. ⚡ Modo Automático Completo\n",
    "\n",
    "Ejecuta todo el proceso automáticamente: generación de datos + entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0276e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para ejecutar todo el proceso automáticamente\n",
    "def run_complete_auto_pipeline():\n",
    "    \"\"\"Ejecuta el pipeline completo: generación de datos + entrenamiento automático\"\"\"\n",
    "    print(\"🚀 Iniciando Pipeline Automático Completo\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Paso 1: Generar datasets si no existen\n",
    "    print(\"📊 PASO 1: Generación de Datasets\")\n",
    "    datasets_ok = generate_datasets_if_needed()\n",
    "\n",
    "    if not datasets_ok:\n",
    "        print(\"❌ Error en generación de datasets. Abortando pipeline.\")\n",
    "        return False\n",
    "\n",
    "    # Re-explorar datasets\n",
    "    global datasets_info\n",
    "    datasets_info = explore_datasets()\n",
    "\n",
    "    if not datasets_info:\n",
    "        print(\"❌ No se encontraron datasets después de la generación. Abortando pipeline.\")\n",
    "        return False\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🤖 PASO 2: Entrenamiento Automático\")\n",
    "    # Paso 2: Entrenamiento automático\n",
    "    model_path = train_model_auto()\n",
    "\n",
    "    if model_path:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"✅ PIPELINE COMPLETADO EXITOSAMENTE!\")\n",
    "        print(f\"📁 Modelo guardado en: {model_path}\")\n",
    "        print(\"\\n🎯 El modelo está listo para usar en inferencia!\")\n",
    "        print(\"💡 Puedes usar el modelo con: python main.py seismic\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ Error en el entrenamiento. Pipeline incompleto.\")\n",
    "        return False\n",
    "\n",
    "# Botón para pipeline completo automático\n",
    "complete_auto_button = widgets.Button(\n",
    "    description='🚀 Pipeline Completo Automático',\n",
    "    button_style='primary',\n",
    "    tooltip='Generar datos + entrenar automáticamente (todo en uno)',\n",
    "    layout=widgets.Layout(width='300px', height='50px')\n",
    ")\n",
    "\n",
    "complete_output_area = widgets.Output()\n",
    "\n",
    "def complete_auto_click(b):\n",
    "    with complete_output_area:\n",
    "        clear_output(wait=True)\n",
    "        run_complete_auto_pipeline()\n",
    "\n",
    "complete_auto_button.on_click(complete_auto_click)\n",
    "\n",
    "# Mostrar botón de pipeline completo\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>⚡ Pipeline Automático Completo</h3>\"),\n",
    "    widgets.HTML(\"🔧 Genera datasets sintéticos automáticamente<br>🤖 Entrena modelo con parámetros optimizados<br>📊 Visualiza resultados completos\"),\n",
    "    widgets.HTML(\"<br><strong>¡Solo presiona el botón y espera!</strong>\"),\n",
    "    complete_auto_button,\n",
    "    complete_output_area\n",
    "]))\n",
    "\n",
    "print(\"🎯 El notebook está listo para usar en modo automático o interactivo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95a2ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 Estado del Sistema\n",
    "\n",
    "Ejecuta esta celda para verificar el estado actual del sistema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99749d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificación del estado del sistema\n",
    "def check_system_status():\n",
    "    \"\"\"Verifica el estado completo del sistema de entrenamiento sísmico\"\"\"\n",
    "    print(\"🔍 Verificando estado del sistema...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Verificar backend\n",
    "    backend_ok = os.path.exists(backend_dir)\n",
    "    print(f\"📁 Backend: {'✅' if backend_ok else '❌'} ({backend_dir})\")\n",
    "\n",
    "    # Verificar datasets\n",
    "    datasets_dir = os.path.join(backend_dir, 'datasets')\n",
    "    datasets_ok = os.path.exists(datasets_dir)\n",
    "    dataset_files = []\n",
    "    if datasets_ok:\n",
    "        dataset_files = [f for f in os.listdir(datasets_dir) if f.endswith('.h5')]\n",
    "    print(f\"📊 Datasets: {'✅' if datasets_ok and dataset_files else '❌'} ({len(dataset_files)} archivos)\")\n",
    "\n",
    "    # Verificar modelos\n",
    "    models_dir = os.path.join(backend_dir, 'models')\n",
    "    models_ok = os.path.exists(models_dir)\n",
    "    print(f\"🤖 Modelos: {'✅' if models_ok else '❌'} ({models_dir})\")\n",
    "\n",
    "    # Verificar checkpoints\n",
    "    checkpoints_dir = os.path.join(backend_dir, 'checkpoints')\n",
    "    checkpoints_ok = os.path.exists(checkpoints_dir)\n",
    "    checkpoint_files = []\n",
    "    if checkpoints_ok:\n",
    "        checkpoint_files = [f for f in os.listdir(checkpoints_dir) if f.endswith('.pth')]\n",
    "    print(f\"💾 Checkpoints: {'✅' if checkpoints_ok else '❌'} ({len(checkpoint_files)} modelos)\")\n",
    "\n",
    "    # Verificar PyTorch y CUDA\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    device_count = torch.cuda.device_count() if cuda_available else 0\n",
    "    print(f\"🖥️  PyTorch: ✅ (CUDA: {'✅' if cuda_available else '❌'}, GPUs: {device_count})\")\n",
    "\n",
    "    # Verificar imports críticos\n",
    "    imports_ok = True\n",
    "    try:\n",
    "        from model_architecture import create_model\n",
    "        from model_training.train_model import DeformationDataset\n",
    "        print(\"📚 Imports: ✅ (model_architecture, train_model)\")\n",
    "    except ImportError as e:\n",
    "        print(f\"📚 Imports: ❌ ({e})\")\n",
    "        imports_ok = False\n",
    "\n",
    "    # Resumen\n",
    "    all_ok = backend_ok and imports_ok\n",
    "    datasets_ready = datasets_ok and dataset_files\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"📋 RESUMEN DEL SISTEMA:\")\n",
    "    print(f\"   Backend configurado: {'✅' if all_ok else '❌'}\")\n",
    "    print(f\"   Datasets listos: {'✅' if datasets_ready else '❌'}\")\n",
    "    print(f\"   Entrenamiento posible: {'✅' if all_ok and (datasets_ready or True) else '❌'}\")\n",
    "\n",
    "    if all_ok and not datasets_ready:\n",
    "        print(\"   💡 Los datasets se generarán automáticamente en modo automático\")\n",
    "    elif all_ok and datasets_ready:\n",
    "        print(\"   🎯 Sistema completamente listo para entrenamiento!\")\n",
    "\n",
    "    return all_ok\n",
    "\n",
    "# Ejecutar verificación\n",
    "system_ready = check_system_status()\n",
    "\n",
    "if system_ready:\n",
    "    print(\"\\n🎉 ¡Sistema listo! Elige tu modo de entrenamiento:\")\n",
    "    print(\"   🚀 Pipeline Completo Automático (recomendado)\")\n",
    "    print(\"   🎛️  Modo Interactivo (control total)\")\n",
    "    print(\"   🤖 Entrenamiento Automático (parámetros optimizados)\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Sistema necesita configuración. Verifica los errores arriba.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
